Vocabluary size: 10000
==================================================
|                    Training                    |
==================================================
Epoch Size=  5311
Percentage Done: 0.2%    |  Perplexity:  9736.87     |   Speed:   218.09 wps
Percentage Done: 10.2%    |  Perplexity:  1641.59     |   Speed:   189.33 wps
Percentage Done: 20.2%    |  Perplexity:  1282.83     |   Speed:   189.47 wps
Percentage Done: 30.2%    |  Perplexity:  1149.49     |   Speed:   191.90 wps
Percentage Done: 40.2%    |  Perplexity:  1073.82     |   Speed:   193.02 wps
Percentage Done: 50.2%    |  Perplexity:  1021.13     |   Speed:   191.57 wps
Percentage Done: 60.2%    |  Perplexity:   979.62     |   Speed:   191.59 wps
Percentage Done: 70.2%    |  Perplexity:   950.89     |   Speed:   191.71 wps
Percentage Done: 80.2%    |  Perplexity:   926.28     |   Speed:   190.57 wps
Percentage Done: 90.2%    |  Perplexity:   909.45     |   Speed:   189.93 wps
Train perplexity at epoch 0:   891.73
Epoch Size=  421
Validation perplexity at epoch 0:   719.78
Epoch Size=  5311
Percentage Done: 0.2%    |  Perplexity:   864.71     |   Speed:   170.35 wps
Traceback (most recent call last):
  File "main.py", line 160, in <module>
    train_p = run_epoch(model, train_data, True, lr)
  File "main.py", line 112, in run_epoch
    loss.backward()
  File "C:\Users\ltopuser\.conda\envs\master\lib\site-packages\torch\tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "C:\Users\ltopuser\.conda\envs\master\lib\site-packages\torch\autograd\__init__.py", line 98, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt

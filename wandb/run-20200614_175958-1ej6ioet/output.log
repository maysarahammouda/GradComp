Vocabluary size: 39
==================================================
|                    Training                    |
==================================================
Epoch Size=  4
batch######## 1
batch######## 2
***batch_idx***: 2
SGD (
Parameter Group 0
    dampening: 0
    lr: 1.0
    momentum: 0
    nesterov: False
    weight_decay: 0
)
LSTM(
  (dropout): Dropout(p=0.5, inplace=False)
  (word_embeddings): Embedding(39, 30)
  (lstm): LSTM(30, 30, num_layers=2, dropout=0.5)
  (sm_fc): Linear(in_features=30, out_features=39, bias=True)
)
batch######## 3
batch######## 4
***batch_idx***: 4
SGD (
Parameter Group 0
    dampening: 0
    lr: 1.0
    momentum: 0
    nesterov: False
    weight_decay: 0
)
LSTM(
  (dropout): Dropout(p=0.5, inplace=False)
  (word_embeddings): Embedding(39, 30)
  (lstm): LSTM(30, 30, num_layers=2, dropout=0.5)
  (sm_fc): Linear(in_features=30, out_features=39, bias=True)
)
Train perplexity at epoch 0:    38.78
Epoch Size=  4
Validation perplexity at epoch 0:    36.26
==================================================
|                    Testing                     |
==================================================
Epoch Size=  36
Test Perplexity:    36.23

======================== Done! ========================

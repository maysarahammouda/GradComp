['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Version', 'of', 'the', 'Bible', '<eos>']
{'of': 0, 'the': 1, '<eos>': 2, 'Bible': 3, 'James': 4, 'King': 5, 'Old': 6, 'Testament': 7, 'The': 8, 'Version': 9}
Vocabluary size: 10
==================================================
|                    Training                    |
==================================================
len of raw data: 12
batch_len: 1
Epoch Size=  0
Traceback (most recent call last):
  File "main.py", line 183, in <module>
    train_ppl = run_epoch(model, train_data, True, lr)
  File "main.py", line 90, in run_epoch
    for batch_idx, (input, target) in enumerate(batch_generator(data, model.batch_size, model.num_steps)):
  File "C:\Users\ltopuser\GradComp\utils.py", line 165, in batch_generator
    raise ValueError("epoch_size == 0, decrease batch_size or num_steps")
ValueError: epoch_size == 0, decrease batch_size or num_steps

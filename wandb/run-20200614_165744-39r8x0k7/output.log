['And', 'God', 'said,', 'Let', 'there', 'be', 'light:', 'and', 'there', 'was', 'light.', '<eos>']
{'there': 0, '<eos>': 1, 'And': 2, 'God': 3, 'Let': 4, 'and': 5, 'be': 6, 'light.': 7, 'light:': 8, 'said,': 9, 'was': 10}
Vocabluary size: 11
==================================================
|                    Training                    |
==================================================
len of raw data: 12
batch_len: 1
Epoch Size=  0
Traceback (most recent call last):
  File "main.py", line 183, in <module>
    train_ppl = run_epoch(model, train_data, True, lr)
  File "main.py", line 90, in run_epoch
    for batch_idx, (input, target) in enumerate(batch_generator(data, model.batch_size, model.num_steps)):
  File "C:\Users\ltopuser\GradComp\utils.py", line 165, in batch_generator
    raise ValueError("epoch_size == 0, decrease batch_size or num_steps")
ValueError: epoch_size == 0, decrease batch_size or num_steps
